{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12ce77a9-23aa-4c3b-8189-f61c20880302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (10/16/2023, Monday, Quirino Avenue)\n",
    "north_data = pd.read_csv('./data/north_data.csv')\n",
    "south_data = pd.read_csv('./data/south_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "483e5b5e-787a-43d7-9db9-8c45e7e4b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Vehicles (per type)\n",
      "1: 8739\n",
      "2: 25339\n",
      "3: 2922\n",
      "4: 5047\n",
      "5: 481\n",
      "6: 70\n",
      "7: 7\n",
      "8: 115\n",
      "9: 269\n",
      "10: 22\n",
      "11: 0\n",
      "12: 0\n",
      "13: 0\n",
      "14: 0\n",
      "15: 1484\n",
      "\n",
      "Percentage of Vehicle Types\n",
      "1: 0.20\n",
      "2: 0.57\n",
      "3: 0.07\n",
      "4: 0.11\n",
      "5: 0.01\n",
      "6: 0.00\n",
      "7: 0.00\n",
      "8: 0.00\n",
      "9: 0.01\n",
      "10: 0.00\n",
      "11: 0.00\n",
      "12: 0.00\n",
      "13: 0.00\n",
      "14: 0.00\n",
      "15: 0.03\n",
      "\n",
      "Total number of vehicles this day: 44495\n",
      "\n",
      "Percentage: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Track the total number of vehicles\n",
    "total_vehicles_by_type = {}\n",
    "total_vehicles_percent = {}\n",
    "total_vehicles = 0\n",
    "total_percent = 0\n",
    "\n",
    "print('Total Number of Vehicles (per type)')\n",
    "for columns in range(15):\n",
    "    col = str(columns + 1)\n",
    "    total_vehicles_by_type[col] = north_data[col].sum() + south_data[col].sum()\n",
    "    print(f'{col}: {total_vehicles_by_type[col]}')\n",
    "print()\n",
    "\n",
    "for v in total_vehicles_by_type.values():\n",
    "    total_vehicles += v\n",
    "\n",
    "print('Percentage of Vehicle Types')\n",
    "for k,v in total_vehicles_by_type.items():\n",
    "    total_vehicles_percent[k] = (v / total_vehicles)\n",
    "    total_percent += total_vehicles_percent[k]\n",
    "    print(f'{k}: {total_vehicles_percent[k]:.2f}')\n",
    "print()\n",
    "\n",
    "print(f'Total number of vehicles this day: {total_vehicles}\\n')\n",
    "print(f'Percentage: {total_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fd3fd4c-7891-4719-9e39-b6f75c2f2ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated routes file with CORRECTED paths: traffic_routes.rou.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. CONFIGURATION: YOU MUST EDIT THIS SECTION ---\n",
    "\n",
    "# Input Data Files\n",
    "NORTH_CSV = \"./data/north_data.csv\"  # Your cleaned Northbound data\n",
    "SOUTH_CSV = \"./data/south_data.csv\"  # Your cleaned Southbound data\n",
    "\n",
    "# Output File\n",
    "OUTPUT_ROUTES_FILE = \"traffic_routes.rou.xml\"\n",
    "\n",
    "# Simulation Parameters\n",
    "SIMULATION_END_TIME = 86400  # 24 hours in seconds\n",
    "\n",
    "# --- CRITICAL: UPDATE WITH YOUR REAL EDGE IDs FROM NETEDIT ---\n",
    "# This is the map you created in Step 1.\n",
    "# The names are examples; yours will be different!\n",
    "EDGES = {\n",
    "    \"north_entry\": \"north_start\",\n",
    "    \"north_exit\": \"north_end\",\n",
    "    \"south_entry\": \"south_start\",\n",
    "    \"south_exit\": \"south_end\",\n",
    "    \"east_entry\": \"east_start\",\n",
    "    \"east_exit\": \"east_end\",\n",
    "    \"west_entry\": \"west_start\",\n",
    "    \"west_exit\": \"west_end\",\n",
    "}\n",
    "\n",
    "# --- 2. NEW TURNING ASSUMPTIONS ---\n",
    "# (Fraction of vehicles that continue straight vs. turn)\n",
    "PROB_STRAIGHT = 0.90\n",
    "PROB_TURN = 0.10\n",
    "SIDE_ROAD_FACTOR = 0.25 # Side road traffic as a fraction of main road\n",
    "\n",
    "# --- 3. SCRIPT LOGIC ---\n",
    "def generate_routes():\n",
    "    try:\n",
    "        north_df = pd.read_csv(NORTH_CSV)\n",
    "        south_df = pd.read_csv(SOUTH_CSV)\n",
    "        north_hourly = north_df.iloc[:, -1]\n",
    "        south_hourly = south_df.iloc[:, -1]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure your CSV files are in the correct location.\")\n",
    "        return\n",
    "\n",
    "    with open(OUTPUT_ROUTES_FILE, \"w\") as routes:\n",
    "        routes.write(\"<routes>\\n\")\n",
    "\n",
    "        # --- Define the VALID routes based on your list ---\n",
    "        # Note: These now include intermediate edges. UPDATE THEM!\n",
    "        routes.write(f'    <route id=\"N_to_N\" edges=\"{EDGES[\"north_entry\"]} {EDGES[\"north_exit\"]}\"/>\\n')\n",
    "        routes.write(f'    <route id=\"S_to_S\" edges=\"{EDGES[\"south_entry\"]} {EDGES[\"south_exit\"]}\"/>\\n')\n",
    "        routes.write(f'    <route id=\"E_to_E\" edges=\"{EDGES[\"east_entry\"]} {EDGES[\"east_exit\"]}\"/>\\n')\n",
    "        routes.write(f'    <route id=\"W_to_W\" edges=\"{EDGES[\"west_entry\"]} {EDGES[\"west_exit\"]}\"/>\\n\\n')\n",
    "        \n",
    "        routes.write(f'    <route id=\"W_to_N\" edges=\"{EDGES[\"west_entry\"]} {EDGES[\"north_exit\"]}\"/>\\n')\n",
    "        routes.write(f'    <route id=\"E_to_S\" edges=\"{EDGES[\"east_entry\"]} {EDGES[\"south_exit\"]}\"/>\\n')\n",
    "        routes.write(f'    <route id=\"S_to_W\" edges=\"{EDGES[\"south_entry\"]} {EDGES[\"west_exit\"]}\"/>\\n')\n",
    "        # Add a route for North to East if that's the correct turn\n",
    "        routes.write(f'    <route id=\"N_to_E\" edges=\"{EDGES[\"north_entry\"]} {EDGES[\"east_exit\"]}\"/>\\n\\n')\n",
    "\n",
    "\n",
    "        # --- Generate hourly flows with correct turning logic ---\n",
    "        for hour in range(24):\n",
    "            begin_time = hour * 3600\n",
    "            end_time = (hour + 1) * 3600\n",
    "\n",
    "            n_vehicles = north_hourly.get(hour, 0)\n",
    "            s_vehicles = south_hourly.get(hour, 0)\n",
    "            avg_main_road = (n_vehicles + s_vehicles) / 2\n",
    "            ew_vehicles = int(avg_main_road * SIDE_ROAD_FACTOR)\n",
    "\n",
    "            # Northbound traffic splits between continuing North and turning East\n",
    "            if n_vehicles > 0:\n",
    "                routes.write(f'    <flow id=\"n_n_{hour}\" route=\"N_to_N\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(n_vehicles * PROB_STRAIGHT)}\"/>\\n')\n",
    "                routes.write(f'    <flow id=\"n_e_{hour}\" route=\"N_to_E\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(n_vehicles * PROB_TURN)}\"/>\\n')\n",
    "            \n",
    "            # Southbound traffic splits between continuing South and turning West\n",
    "            if s_vehicles > 0:\n",
    "                routes.write(f'    <flow id=\"s_s_{hour}\" route=\"S_to_S\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(s_vehicles * PROB_STRAIGHT)}\"/>\\n')\n",
    "                routes.write(f'    <flow id=\"s_w_{hour}\" route=\"S_to_W\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(s_vehicles * PROB_TURN)}\"/>\\n')\n",
    "\n",
    "            # Eastbound and Westbound traffic\n",
    "            if ew_vehicles > 0:\n",
    "                 routes.write(f'    <flow id=\"e_e_{hour}\" route=\"E_to_E\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(ew_vehicles * PROB_STRAIGHT)}\"/>\\n')\n",
    "                 routes.write(f'    <flow id=\"e_s_{hour}\" route=\"E_to_S\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(ew_vehicles * PROB_TURN)}\"/>\\n')\n",
    "                 routes.write(f'    <flow id=\"w_w_{hour}\" route=\"W_to_W\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(ew_vehicles * PROB_STRAIGHT)}\"/>\\n')\n",
    "                 routes.write(f'    <flow id=\"w_n_{hour}\" route=\"W_to_N\" type=\"realdistribution\" begin=\"{begin_time}\" end=\"{end_time}\" number=\"{int(ew_vehicles * PROB_TURN)}\"/>\\n')\n",
    "\n",
    "        routes.write(\"</routes>\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_routes()\n",
    "    print(f\"Successfully generated routes file with CORRECTED paths: {OUTPUT_ROUTES_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c00912-e376-45bd-b7a3-d38cb50cdc36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "lxml not found, please install or use the etree parser.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- Main Analysis ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m baseline_metrics = \u001b[43manalyze_tripinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtripinfo.xml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m rl_agent_metrics = analyze_tripinfo(\u001b[33m\"\u001b[39m\u001b[33mrl_trip_info.xml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m baseline_metrics \u001b[38;5;129;01mand\u001b[39;00m rl_agent_metrics:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# 1. Print a comparison table\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36manalyze_tripinfo\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parses a tripinfo file and returns a dictionary of key metrics.\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     metrics = {\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThroughput\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[32m     11\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAvg. Wait Time (s)\u001b[39m\u001b[33m'\u001b[39m: df[\u001b[33m'\u001b[39m\u001b[33mwaitingTime\u001b[39m\u001b[33m'\u001b[39m].mean(),\n\u001b[32m     12\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAvg. Time Loss (s)\u001b[39m\u001b[33m'\u001b[39m: df[\u001b[33m'\u001b[39m\u001b[33mtimeLoss\u001b[39m\u001b[33m'\u001b[39m].mean(),\n\u001b[32m     13\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAvg. Trip Duration (s)\u001b[39m\u001b[33m'\u001b[39m: df[\u001b[33m'\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m'\u001b[39m].mean()\n\u001b[32m     14\u001b[39m     }\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/6jjk1sb13ybmp88h0cwdc0jxcpaxc3ws-python3-3.13.8-env/lib/python3.13/site-packages/pandas/io/xml.py:1160\u001b[39m, in \u001b[36mread_xml\u001b[39m\u001b[34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, dtype, converters, parse_dates, encoding, parser, stylesheet, iterparse, compression, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    888\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[33;03mRead XML document into a :class:`~pandas.DataFrame` object.\u001b[39;00m\n\u001b[32m    890\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1156\u001b[39m \u001b[33;03m1      1  <NA>  4.5  False  b 2019-12-31\u001b[39;00m\n\u001b[32m   1157\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1158\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43melems_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43melems_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstylesheet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstylesheet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/6jjk1sb13ybmp88h0cwdc0jxcpaxc3ws-python3-3.13.8-env/lib/python3.13/site-packages/pandas/io/xml.py:830\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, dtype, converters, parse_dates, encoding, parser, stylesheet, iterparse, compression, storage_options, dtype_backend, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m         p = _LxmlFrameParser(\n\u001b[32m    814\u001b[39m             path_or_buffer,\n\u001b[32m    815\u001b[39m             xpath,\n\u001b[32m   (...)\u001b[39m\u001b[32m    827\u001b[39m             storage_options,\n\u001b[32m    828\u001b[39m         )\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mlxml not found, please install or use the etree parser.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m parser == \u001b[33m\"\u001b[39m\u001b[33metree\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    833\u001b[39m     p = _EtreeFrameParser(\n\u001b[32m    834\u001b[39m         path_or_buffer,\n\u001b[32m    835\u001b[39m         xpath,\n\u001b[32m   (...)\u001b[39m\u001b[32m    847\u001b[39m         storage_options,\n\u001b[32m    848\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: lxml not found, please install or use the etree parser."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_tripinfo(filepath):\n",
    "    \"\"\"Parses a tripinfo file and returns a dictionary of key metrics.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_xml(filepath)\n",
    "        metrics = {\n",
    "            'Throughput': len(df),\n",
    "            'Avg. Wait Time (s)': df['waitingTime'].mean(),\n",
    "            'Avg. Time Loss (s)': df['timeLoss'].mean(),\n",
    "            'Avg. Trip Duration (s)': df['duration'].mean()\n",
    "        }\n",
    "        return metrics\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filepath} not found.\")\n",
    "        return None\n",
    "\n",
    "# --- Main Analysis ---\n",
    "baseline_metrics = analyze_tripinfo(\"tripinfo.xml\")\n",
    "rl_agent_metrics = analyze_tripinfo(\"rl_trip_info.xml\")\n",
    "\n",
    "if baseline_metrics and rl_agent_metrics:\n",
    "    # 1. Print a comparison table\n",
    "    print(\"--- Performance Comparison ---\")\n",
    "    df_compare = pd.DataFrame({'Baseline': baseline_metrics, 'RL Agent': rl_agent_metrics})\n",
    "    print(df_compare.round(2))\n",
    "\n",
    "    # 2. Visualize the comparison with a bar chart\n",
    "    labels = list(baseline_metrics.keys())\n",
    "    baseline_values = list(baseline_metrics.values())\n",
    "    rl_values = list(rl_agent_metrics.values())\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    rects1 = ax.bar(x - width/2, baseline_values, width, label='Baseline (Fixed-Time)')\n",
    "    rects2 = ax.bar(x + width/2, rl_values, width, label='RL Agent')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Performance Comparison: Baseline vs. RL Agent')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    ax.bar_label(rects1, padding=3, fmt='%.1f')\n",
    "    ax.bar_label(rects2, padding=3, fmt='%.1f')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
